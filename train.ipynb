{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3962e991",
   "metadata": {},
   "source": [
    "# Moral Machine Trolley Problem AI\n",
    "This notebook trains a PyTorch model to decide between two trolley routes based on the Moral Machine dataset.\n",
    "\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef81173a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03e135ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = './dataset/SharedResponses.csv'\n",
    "N_ROWS = None  # Load all rows from the paired file\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 5\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287922c9",
   "metadata": {},
   "source": [
    "### Filtering and generation of subset to train on\n",
    "\n",
    "We create a subset of the dataset to train on, as the full dataset is quite large (11GB+). We will load only the first 200,000 rows for training. \n",
    "\n",
    "We also pair the by scenario responses together, so that the model can learn to compare the two routes directly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef87f0",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Preprocessing (Two-Pass)\n",
    "\n",
    "Since the dataset rows for a single scenario (\"ResponseID\") might be far apart in the file, and we have limited RAM, we cannot rely on simple chunking because a chunk might contain only one half of the pair.\n",
    "\n",
    "We will use a **Two-Pass Strategy**:\n",
    "1.  **Pass 1 (Lightweight)**: Scan only the `ResponseID` column to identify which IDs represent complete pairs. We stop once we have found enough target pairs (e.g., 300,000).\n",
    "2.  **Pass 2 (Extraction)**: identifying the IDs we want, we scan the file again to load the full data *only* for those IDs.\n",
    "\n",
    "This ensures we get complete pairs without loading the 11GB file into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfee20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- PASS 1: Identifying 300000 valid pairs from ./dataset/SharedResponses.csv ---\n",
      "Scanned chunk 5. Found 0 pairs so far...\n",
      "Scanned chunk 10. Found 0 pairs so far...\n",
      "Scanned chunk 15. Found 0 pairs so far...\n",
      "Target of 300000 pairs reached at chunk 19.\n",
      "Pass 1 Complete. Found total 300000 pairs to extract.\n",
      "\n",
      "--- PASS 2: Extracting data for 300000 pairs ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\urbai\\AppData\\Local\\Temp\\ipykernel_25372\\2078326083.py:49: DtypeWarning: Columns (0: Man) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for i, chunk in enumerate(pd.read_csv(filename, chunksize=CHUNK_SIZE)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 10...\n",
      "Processing chunk 20...\n",
      "Processing chunk 30...\n",
      "Processing chunk 40...\n",
      "Processing chunk 50...\n",
      "Processing chunk 60...\n",
      "Processing chunk 70...\n",
      "Processing chunk 80...\n",
      "Processing chunk 90...\n",
      "Processing chunk 100...\n",
      "Processing chunk 110...\n",
      "Processing chunk 120...\n",
      "Processing chunk 130...\n",
      "Processing chunk 140...\n",
      "Concatenating extracted rows...\n",
      "Subset shape: (600000, 41)\n",
      "Creating paired dataset...\n",
      "Final training set scenarios: 300000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ExtendedSessionID_A</th>\n",
       "      <th>UserID_A</th>\n",
       "      <th>ScenarioOrder_A</th>\n",
       "      <th>Intervention_A</th>\n",
       "      <th>PedPed_A</th>\n",
       "      <th>Barrier_A</th>\n",
       "      <th>CrossingSignal_A</th>\n",
       "      <th>AttributeLevel_A</th>\n",
       "      <th>ScenarioTypeStrict_A</th>\n",
       "      <th>ScenarioType_A</th>\n",
       "      <th>...</th>\n",
       "      <th>MaleExecutive_B</th>\n",
       "      <th>FemaleExecutive_B</th>\n",
       "      <th>FemaleAthlete_B</th>\n",
       "      <th>MaleAthlete_B</th>\n",
       "      <th>FemaleDoctor_B</th>\n",
       "      <th>MaleDoctor_B</th>\n",
       "      <th>Dog_B</th>\n",
       "      <th>Cat_B</th>\n",
       "      <th>sub_id_B</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ResponseID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2222bRQqBTZ6dLnPH</th>\n",
       "      <td>32757157_6999801415950060.0</td>\n",
       "      <td>6.999801e+15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Fit</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>Fitness</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222sJk4DcoqXXi98</th>\n",
       "      <td>1043988516_3525281295.0</td>\n",
       "      <td>3.525281e+09</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Rand</td>\n",
       "      <td>Random</td>\n",
       "      <td>Random</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223CNmvTr2Coj4wp</th>\n",
       "      <td>-1613944085_422160228641876.0</td>\n",
       "      <td>4.221602e+14</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Female</td>\n",
       "      <td>Gender</td>\n",
       "      <td>Gender</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223Xu54ufgjcyMR3</th>\n",
       "      <td>1425316635_327833569077076.0</td>\n",
       "      <td>3.278336e+14</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Old</td>\n",
       "      <td>Age</td>\n",
       "      <td>Age</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223jMWDEGNeszivb</th>\n",
       "      <td>-1683127088_785070916172117.0</td>\n",
       "      <td>7.850709e+14</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>More</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>Utilitarian</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ExtendedSessionID_A      UserID_A  \\\n",
       "ResponseID                                                       \n",
       "2222bRQqBTZ6dLnPH    32757157_6999801415950060.0  6.999801e+15   \n",
       "2222sJk4DcoqXXi98        1043988516_3525281295.0  3.525281e+09   \n",
       "2223CNmvTr2Coj4wp  -1613944085_422160228641876.0  4.221602e+14   \n",
       "2223Xu54ufgjcyMR3   1425316635_327833569077076.0  3.278336e+14   \n",
       "2223jMWDEGNeszivb  -1683127088_785070916172117.0  7.850709e+14   \n",
       "\n",
       "                   ScenarioOrder_A  Intervention_A  PedPed_A  Barrier_A  \\\n",
       "ResponseID                                                                \n",
       "2222bRQqBTZ6dLnPH                7               0         0          0   \n",
       "2222sJk4DcoqXXi98                2               1         0          1   \n",
       "2223CNmvTr2Coj4wp               10               0         1          0   \n",
       "2223Xu54ufgjcyMR3               11               0         0          1   \n",
       "2223jMWDEGNeszivb                8               0         1          0   \n",
       "\n",
       "                   CrossingSignal_A AttributeLevel_A ScenarioTypeStrict_A  \\\n",
       "ResponseID                                                                  \n",
       "2222bRQqBTZ6dLnPH                 1              Fit              Fitness   \n",
       "2222sJk4DcoqXXi98                 0             Rand               Random   \n",
       "2223CNmvTr2Coj4wp                 1           Female               Gender   \n",
       "2223Xu54ufgjcyMR3                 0              Old                  Age   \n",
       "2223jMWDEGNeszivb                 2             More          Utilitarian   \n",
       "\n",
       "                  ScenarioType_A  ... MaleExecutive_B FemaleExecutive_B  \\\n",
       "ResponseID                        ...                                     \n",
       "2222bRQqBTZ6dLnPH        Fitness  ...             0.0               0.0   \n",
       "2222sJk4DcoqXXi98         Random  ...             1.0               0.0   \n",
       "2223CNmvTr2Coj4wp         Gender  ...             1.0               0.0   \n",
       "2223Xu54ufgjcyMR3            Age  ...             0.0               0.0   \n",
       "2223jMWDEGNeszivb    Utilitarian  ...             1.0               0.0   \n",
       "\n",
       "                   FemaleAthlete_B  MaleAthlete_B  FemaleDoctor_B  \\\n",
       "ResponseID                                                          \n",
       "2222bRQqBTZ6dLnPH              0.0            0.0             0.0   \n",
       "2222sJk4DcoqXXi98              0.0            0.0             0.0   \n",
       "2223CNmvTr2Coj4wp              0.0            0.0             0.0   \n",
       "2223Xu54ufgjcyMR3              0.0            0.0             0.0   \n",
       "2223jMWDEGNeszivb              0.0            0.0             0.0   \n",
       "\n",
       "                   MaleDoctor_B Dog_B  Cat_B  sub_id_B Label  \n",
       "ResponseID                                                    \n",
       "2222bRQqBTZ6dLnPH           0.0   0.0    0.0         1     0  \n",
       "2222sJk4DcoqXXi98           0.0   0.0    0.0         1     1  \n",
       "2223CNmvTr2Coj4wp           0.0   0.0    0.0         1     0  \n",
       "2223Xu54ufgjcyMR3           0.0   0.0    0.0         1     1  \n",
       "2223jMWDEGNeszivb           0.0   1.0    0.0         1     1  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "# Configuration\n",
    "TARGET_PAIRS = 300000\n",
    "CHUNK_SIZE = 500000 # Larger chunks for faster iteration\n",
    "filename = DATA_PATH\n",
    "\n",
    "print(f\"--- PASS 1: Identifying {TARGET_PAIRS} valid pairs from {filename} ---\")\n",
    "\n",
    "seen_ids = set()\n",
    "wanted_ids = set()\n",
    "\n",
    "# Iterate ONLY over ResponseID column to save memory\n",
    "# We only need to find IDs that appear twice\n",
    "for i, chunk in enumerate(pd.read_csv(filename, usecols=['ResponseID'], chunksize=CHUNK_SIZE)):\n",
    "    chunk_ids = chunk['ResponseID'].tolist()\n",
    "    \n",
    "    for rid in chunk_ids:\n",
    "        if rid in seen_ids:\n",
    "            # We found the second part of the pair!\n",
    "            wanted_ids.add(rid)\n",
    "            # Remove from seen to keep memory usage stable (assuming max 2 rows per ID)\n",
    "            seen_ids.remove(rid)\n",
    "            \n",
    "            if len(wanted_ids) >= TARGET_PAIRS:\n",
    "                break\n",
    "        else:\n",
    "            seen_ids.add(rid)\n",
    "            \n",
    "    if len(wanted_ids) >= TARGET_PAIRS:\n",
    "        print(f\"Target of {TARGET_PAIRS} pairs reached at chunk {i+1}.\")\n",
    "        break\n",
    "    \n",
    "    if (i + 1) % 5 == 0:\n",
    "        print(f\"Scanned chunk {i+1}. Found {len(wanted_ids)} pairs so far...\")\n",
    "\n",
    "print(f\"Pass 1 Complete. Found total {len(wanted_ids)} pairs to extract.\")\n",
    "\n",
    "# Clear temporary set to free memory\n",
    "del seen_ids\n",
    "gc.collect()\n",
    "\n",
    "print(f\"\\n--- PASS 2: Extracting data for {len(wanted_ids)} pairs ---\")\n",
    "# Now we read the full file, but only keep rows belonging to wanted_ids\n",
    "\n",
    "extracted_rows = []\n",
    "pairs_collected = 0\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(filename, chunksize=CHUNK_SIZE)):\n",
    "    # Filter this chunk for rows that match our wanted IDs\n",
    "    mask = chunk['ResponseID'].isin(wanted_ids)\n",
    "    \n",
    "    if mask.any():\n",
    "        relevant_rows = chunk[mask].copy()\n",
    "        extracted_rows.append(relevant_rows)\n",
    "        # Optimization: verify if we have collected all rows for the wanted pairs\n",
    "        # But since rows are scattered, we likely need to scan further.\n",
    "\n",
    "    # Optional: Progress logging\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processing chunk {i+1}...\")\n",
    "\n",
    "print(\"Concatenating extracted rows...\")\n",
    "if not extracted_rows:\n",
    "    raise ValueError(\"No rows were extracted! Check if the dataset path is correct.\")\n",
    "\n",
    "df_raw_subset = pd.concat(extracted_rows)\n",
    "\n",
    "print(f\"Subset shape: {df_raw_subset.shape}\")\n",
    "print(\"Creating paired dataset...\")\n",
    "\n",
    "# Now proceed with pairing logic on the subset\n",
    "# 1. Sort by ResponseID to ensure A and B are adjacent/grouped\n",
    "df_raw_subset = df_raw_subset.sort_values('ResponseID')\n",
    "\n",
    "# 2. Assign sub_id\n",
    "df_raw_subset['sub_id'] = df_raw_subset.groupby('ResponseID').cumcount()\n",
    "\n",
    "# 3. Split\n",
    "option_a = df_raw_subset[df_raw_subset['sub_id'] == 0].set_index('ResponseID')\n",
    "option_b = df_raw_subset[df_raw_subset['sub_id'] == 1].set_index('ResponseID')\n",
    "\n",
    "# 4. Join\n",
    "paired_df = option_a.join(option_b, lsuffix='_A', rsuffix='_B')\n",
    "\n",
    "# 5. Create Label & Filter\n",
    "paired_df['Label'] = (paired_df['Saved_B'] == 1).astype(int)\n",
    "valid_rows = (paired_df['Saved_A'] + paired_df['Saved_B']) == 1\n",
    "paired_df = paired_df[valid_rows]\n",
    "\n",
    "print(f\"Final training set scenarios: {len(paired_df)}\")\n",
    "paired_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e508e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the paired dataset for future use\n",
    "paired_df.to_csv('./dataset/PairedResponses.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c52a4e",
   "metadata": {},
   "source": [
    "### 3. Model Training\n",
    "We will define a simple feedforward neural network to learn the decision-making process based on the features of the two routes. The model will be trained to predict which route is chosen by the majority of respondents for each scenario.\n",
    "\n",
    "The input features will include:\n",
    "- The attributes of the characters on each route (e.g., number of people, presence of children, etc.)\n",
    "- The country of the respondent is also included as a feature (one-hot encoded), as it may influence moral decisions.\n",
    "\n",
    "The target variable will be a binary label indicating which route was chosen by the majority of respondents for that scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffca83a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features per option: 22\n",
      "Number of unique countries: 197\n",
      "Train samples: 240000, Validation samples: 60000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Define Features\n",
    "CHAR_COLS = ['Man', 'Woman', 'Pregnant', 'Stroller', 'OldMan', 'OldWoman', 'Boy', 'Girl', \n",
    "             'Homeless', 'LargeWoman', 'LargeMan', 'Criminal', 'MaleExecutive', 'FemaleExecutive', \n",
    "             'FemaleAthlete', 'MaleAthlete', 'FemaleDoctor', 'MaleDoctor', 'Dog', 'Cat']\n",
    "\n",
    "CTX_COLS = ['Intervention', 'CrossingSignal']\n",
    "\n",
    "# Construct full feature list for A and B\n",
    "# Note: 'UserCountry3' is user-level, doesn't change between A and B\n",
    "feat_cols_A = [c + '_A' for c in CHAR_COLS + CTX_COLS]\n",
    "feat_cols_B = [c + '_B' for c in CHAR_COLS + CTX_COLS]\n",
    "\n",
    "print(f\"Features per option: {len(feat_cols_A)}\")\n",
    "\n",
    "# 2. Encode Country\n",
    "# Fill NaN countries with 'Unknown'\n",
    "paired_df['UserCountry3'] = paired_df['UserCountry3_A'].fillna('Unknown') # _A and _B are same for country\n",
    "\n",
    "country_encoder = LabelEncoder()\n",
    "paired_df['country_idx'] = country_encoder.fit_transform(paired_df['UserCountry3'].astype(str))\n",
    "n_countries = len(country_encoder.classes_)\n",
    "print(f\"Number of unique countries: {n_countries}\")\n",
    "\n",
    "# 3. Prepare Tensors\n",
    "X_A = paired_df[feat_cols_A].values.astype(np.float32)\n",
    "X_B = paired_df[feat_cols_B].values.astype(np.float32)\n",
    "X_country = paired_df['country_idx'].values.astype(np.int64)\n",
    "y = paired_df['Label'].values.astype(np.float32)\n",
    "\n",
    "# 4. Split Train/Test\n",
    "# We split indices to keep arrays aligned\n",
    "indices = np.arange(len(paired_df))\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train samples: {len(train_idx)}, Validation samples: {len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9b4c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders prepared.\n"
     ]
    }
   ],
   "source": [
    "class MoralDataset(Dataset):\n",
    "    def __init__(self, x_a, x_b, x_country, y):\n",
    "        self.x_a = torch.tensor(x_a, dtype=torch.float32)\n",
    "        self.x_b = torch.tensor(x_b, dtype=torch.float32)\n",
    "        self.x_country = torch.tensor(x_country, dtype=torch.long)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_a[idx], self.x_b[idx], self.x_country[idx], self.y[idx]\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = MoralDataset(X_A[train_idx], X_B[train_idx], X_country[train_idx], y[train_idx])\n",
    "val_dataset = MoralDataset(X_A[val_idx], X_B[val_idx], X_country[val_idx], y[val_idx])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"DataLoaders prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d378a446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrolleyModel(\n",
      "  (country_emb): Embedding(197, 16)\n",
      "  (feature_net): Sequential(\n",
      "    (0): Linear(in_features=38, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class TrolleyModel(nn.Module):\n",
    "    def __init__(self, num_features, num_countries, emb_dim=16):\n",
    "        super(TrolleyModel, self).__init__()\n",
    "        \n",
    "        # Embedding for user country\n",
    "        self.country_emb = nn.Embedding(num_countries, emb_dim)\n",
    "        \n",
    "        # Shared feature extractor (Siamese-like structure)\n",
    "        # We process Option A and Option B through the same weights to learn \"Value of an option\"\n",
    "        # Input dim: num_features + emb_dim (we append country context to both)\n",
    "        input_dim = num_features + emb_dim\n",
    "        \n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)  # Outputs a scalar \"score\" or \"utility\" for the option\n",
    "        )\n",
    "        \n",
    "    def forward(self, x_a, x_b, country_idx):\n",
    "        # Get country embedding\n",
    "        c_emb = self.country_emb(country_idx) # [batch, emb_dim]\n",
    "        \n",
    "        # Concatenate country info to both options\n",
    "        # x_a: [batch, features], c_emb: [batch, emb_dim]\n",
    "        a_input = torch.cat([x_a, c_emb], dim=1)\n",
    "        b_input = torch.cat([x_b, c_emb], dim=1)\n",
    "        \n",
    "        # Compute scores for both options\n",
    "        score_a = self.feature_net(a_input)\n",
    "        score_b = self.feature_net(b_input)\n",
    "        \n",
    "        # Logits for binary classification (Choice B vs A)\n",
    "        # If score_b > score_a, logits > 0, probability > 0.5 -> Choose B\n",
    "        logits = score_b - score_a\n",
    "        return logits\n",
    "\n",
    "model = TrolleyModel(num_features=X_A.shape[1], num_countries=n_countries).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4dc9590c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/5 | Train Loss: 0.5551 Acc: 0.7254 | Val Loss: nan Acc: 0.7173\n",
      "Epoch 2/5 | Train Loss: 0.5548 Acc: 0.7252 | Val Loss: nan Acc: 0.7170\n",
      "Epoch 3/5 | Train Loss: 0.5545 Acc: 0.7261 | Val Loss: nan Acc: 0.7178\n",
      "Epoch 4/5 | Train Loss: 0.5540 Acc: 0.7258 | Val Loss: nan Acc: 0.7183\n",
      "Epoch 5/5 | Train Loss: 0.5537 Acc: 0.7268 | Val Loss: nan Acc: 0.7164\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x_a, x_b, country, labels in loader:\n",
    "        x_a, x_b, country, labels = x_a.to(device), x_b.to(device), country.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_a, x_b, country)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Accuracy\n",
    "        predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_a, x_b, country, labels in loader:\n",
    "            x_a, x_b, country, labels = x_a.to(device), x_b.to(device), country.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(x_a, x_b, country)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "    return total_loss / len(loader), correct / total\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}\")\n",
    "\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4155e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to a file for future use\n",
    "torch.save(model.state_dict(), 'trolley_model_acc0_7.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273cc457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrolleyModel(\n",
       "  (country_emb): Embedding(197, 16)\n",
       "  (feature_net): Sequential(\n",
       "    (0): Linear(in_features=38, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export the model to ONNX format for use in other environments (e.g., JavaScript)\n",
    "dummy_x_a = torch.randn(1, X_A.shape[1]).to(device)\n",
    "dummy_x_b = torch.randn(1, X_B.shape[1]).to(device)\n",
    "dummy_country = torch.tensor([0], dtype=torch.long).to(device)  # Example country\n",
    "\n",
    "torch.onnx.export(model, (dummy_x_a, dummy_x_b, dummy_country), 'trolley_model.onnx',\n",
    "                  input_names=['x_a', 'x_b', 'country_idx'],\n",
    "                    output_names=['logits'],\n",
    "                    dynamic_axes={'x_a': {0: 'batch_size'}, 'x_b': {0: 'batch_size'}, 'country_idx': {0: 'batch_size'}, 'logits': {0: 'batch_size'}})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0a5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
